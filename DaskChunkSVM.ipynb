{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439f166-6c1d-4f46-9ff5-84b0ce132767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.externals import joblib\n",
    "from dask_ml.model_selection import RandomizedSearchCV, train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import heapq\n",
    "import os\n",
    "\n",
    "# Load data using Dask\n",
    "# Setup for reading from S3\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False, key='YOUR_ACCESS_KEY', secret='YOUR_SECRET_KEY')\n",
    "s3_path = \"s3://your-bucket-name/prefix\"\n",
    "df = dd.read_csv(s3_path + \"/*.csv\", storage_options={\"key\": \"YOUR_ACCESS_KEY\", \"secret\": \"YOUR_SECRET_KEY\"})\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('filter_age', axis=1)\n",
    "y = df['filter_age']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "# RBF Kernel approximation\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "\n",
    "# Initialize the SGDClassifier for linear SVM\n",
    "clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "\n",
    "# Hyperparameter optimization using RandomizedSearchCV\n",
    "param_dist = {'alpha': np.logspace(-4, 0, 5),\n",
    "              'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'],\n",
    "              'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=10, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "\n",
    "# Use a subset of data for initial hyperparameter tuning\n",
    "X_sample, y_sample = X_train.sample(frac=0.05).compute(), y_train.sample(frac=0.05).compute()\n",
    "X_sample_features = rbf_feature.fit_transform(X_sample)\n",
    "\n",
    "# Initial hyperparameter search on the sample\n",
    "random_search.fit(X_sample_features, y_sample)\n",
    "best_hyperparameters = random_search.best_params_\n",
    "\n",
    "# Initialize the SGDClassifier with the best found hyperparameters\n",
    "clf = SGDClassifier(max_iter=1000, tol=1e-3, **best_hyperparameters)\n",
    "\n",
    "# Initialize priority queue for top 3 models\n",
    "top_models = []\n",
    "\n",
    "# Train incrementally using Dask chunks\n",
    "for X_chunk, y_chunk in zip(X_train.to_dask_array(lengths=True).blocks, y_train.to_dask_array(lengths=True).blocks):\n",
    "    X_chunk_np = X_chunk.compute()\n",
    "    y_chunk_np = y_chunk.compute()\n",
    "    \n",
    "    # Apply the RBF sampler transform\n",
    "    X_features_chunk = rbf_feature.fit_transform(X_chunk_np)\n",
    "    \n",
    "    # Incremental training using partial_fit\n",
    "    clf.partial_fit(X_features_chunk, y_chunk_np, classes=np.unique(y_chunk_np))\n",
    "    \n",
    "    # Evaluate the model's performance on the current chunk\n",
    "    current_accuracy = accuracy_score(y_chunk_np, clf.predict(X_features_chunk))\n",
    "    \n",
    "    # Save the model if it's one of the top 3\n",
    "    if len(top_models) < 3 or current_accuracy > min([model[0] for model in top_models]):\n",
    "        heapq.heappush(top_models, (current_accuracy, joblib.dump(clf, \"model.pkl\")))\n",
    "        if len(top_models) > 3:\n",
    "            _, model_path = heapq.heappop(top_models)\n",
    "            os.remove(model_path)\n",
    "\n",
    "# Evaluate the best model on the reserved test set using chunks\n",
    "final_model = heapq.nlargest(1, top_models, key=lambda x: x[0])[0][1]\n",
    "accuracies = []\n",
    "\n",
    "for X_chunk, y_chunk in zip(X_test.to_dask_array(lengths=True).blocks, y_test.to_dask_array(lengths=True).blocks):\n",
    "    X_chunk_np = X_chunk.compute()\n",
    "    y_chunk_np = y_chunk.compute()\n",
    "    \n",
    "    # Apply the RBF sampler transform\n",
    "    X_features_chunk = rbf_feature.transform(X_chunk_np)\n",
    "    \n",
    "    y_pred_chunk = final_model.predict(X_features_chunk)\n",
    "    accuracies.append(accuracy_score(y_chunk_np, y_pred_chunk))\n",
    "\n",
    "# Calculate the average accuracy across all chunks\n",
    "final_accuracy = np.mean(accuracies)\n",
    "print(f\"Final Model Accuracy: {final_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd973fd2-b065-4797-a062-1898729709bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling time-series data in machine learning, especially with models like SVM which don't inherently understand sequence data, requires some careful feature engineering to encapsulate the temporal nature of the data. Here's a series of steps and strategies you can consider:\n",
    "\n",
    "1. **Segmentation**: If each \"cycle of driving\" for a car is considered as one sequence, then you need to ensure that each sequence (or segment) is treated as one data point. \n",
    "\n",
    "2. **Statistical Features**:\n",
    "   - For each sequence (driving cycle), you can compute statistical features such as:\n",
    "     - **Mean, Median**: Average value of features over the cycle.\n",
    "     - **Standard Deviation, Variance**: Measure of the spread of the feature values.\n",
    "     - **Min, Max**: Extreme values of the feature in the cycle.\n",
    "     - **Skewness, Kurtosis**: Measure of the shape of the feature distribution.\n",
    "     - **Percentiles**: e.g., 25th, 75th percentile can provide insights into the distribution tails.\n",
    "   \n",
    "3. **Trend Features**:\n",
    "   - **Slope**: You can fit a linear regression model to each feature over the time sequence and use the slope as a feature.\n",
    "   - **Rolling Features**: Compute rolling averages, rolling standard deviations, etc., with different windows (e.g., rolling mean over 10, 50, 100 time steps).\n",
    "\n",
    "4. **Lagged Features**:\n",
    "   - For each feature, you can create lagged features. For instance, you could use the value of 'Temperature' from one, two, or more time steps ago as new features. This helps encapsulate the temporal dependence.\n",
    "\n",
    "5. **Frequency Domain Features**:\n",
    "   - **FFT (Fast Fourier Transform)**: Convert the time-series data to its frequency components.\n",
    "   - **Power Spectral Density**: Represents the distribution of power over frequency.\n",
    "\n",
    "6. **Embeddings from Time-Series Models**:\n",
    "   - Train models like autoencoders on your time-series data and use the embeddings (encoded features) as input features to your SVM.\n",
    "\n",
    "7. **Window-based Features**:\n",
    "   - Instead of treating the entire sequence as one data point, you can create overlapping windows (e.g., of length 50 or 100 time steps) and compute features for each window. This will significantly increase your data points but might give better insights into short-term patterns.\n",
    "\n",
    "8. **Domain-Specific Features**: Depending on domain knowledge about cars and driving patterns, you might derive features that capture specific behaviors or patterns.\n",
    "\n",
    "9. **Feature Scaling**: Time-series data, especially when transformed into multiple statistical features, can span different scales. Use Min-Max scaling or Z-score normalization to ensure features are on a similar scale.\n",
    "\n",
    "10. **Feature Selection**: With many generated features, there's a risk of overfitting. Use techniques like PCA, or feature importance from tree-based models to rank and select the most relevant features.\n",
    "\n",
    "11. **Temporal Train-Test Split**: Ensure that your training set precedes your test set in time to avoid lookahead bias.\n",
    "\n",
    "Remember that SVMs, especially with non-linear kernels, can be computationally intensive. With the addition of many new features, the computational cost can increase significantly. Regularization, feature selection, and using approximations (like the one provided by `RBFSampler`) become even more crucial.\n",
    "\n",
    "Incorporating these feature engineering techniques can help encapsulate the temporal nature of the data and make it more amenable for models like SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4b790-b67c-4069-8251-bf0ac755eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "\n",
    "# Parameters\n",
    "input_sequence_length = 100  # e.g., 100 time steps in each driving cycle\n",
    "n_features = 8  # e.g., 'Temperature', 'Speed', etc.\n",
    "embedding_dim = 32  # Dimension of the embedding, can be tuned\n",
    "\n",
    "# Define the sequence-to-sequence autoencoder\n",
    "\n",
    "inputs = Input(shape=(100000, 8))\n",
    "encoded = LSTM(128, return_sequences=True)(inputs)\n",
    "encoded = LSTM(64, return_sequences=True)(encoded)\n",
    "encoded = LSTM(32)(encoded)\n",
    "\n",
    "decoded = RepeatVector(100000)(encoded)\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "decoded = LSTM(128, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(8, activation='linear'))(decoded)\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Assume X_train_timeseries is your training time-series data of shape (n_samples, input_sequence_length, n_features)\n",
    "autoencoder.fit(X_train_timeseries, X_train_timeseries, \n",
    "                epochs=100, \n",
    "                batch_size=16, \n",
    "                validation_split=0.2, \n",
    "                callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Once trained, use the encoder to get embeddings for your SVM\n",
    "X_train_embeddings = encoder.predict(X_train_timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588acaea-480a-4011-99a5-e41c0f822755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
